{"cells":[{"cell_type":"code","execution_count":null,"id":"0eaf4662-e6f4-431b-a332-304796b41830","metadata":{"id":"0eaf4662-e6f4-431b-a332-304796b41830"},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n","!pip install scipy\n","!pip install tensorboard\n","!pip install huggingface_hub\n","!huggingface-cli login --token '##############'\n","\n","import os\n","import torch\n","from datasets import load_dataset, Dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel, get_peft_model\n","from trl import SFTTrainer\n","\n","# The model that you want to train from the Hugging Face hub\n","model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"squad_v2\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-hf-squad_v2\"\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training\n","bf16 = False\n","fp16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule (constant a bit better than cosine)\n","lr_scheduler_type = \"constant\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 1000\n","\n","# Log every X updates steps\n","logging_steps = 1000\n","\n","# Maximum sequence length to use\n","max_seq_length = 2048\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n","\n","dataset_name = 'squad_v2'\n","train_dataset = load_dataset(dataset_name, split=\"train\")\n","\n","SYSTEM_PROMPT = \"\"\"\\\n","<s>[INST] <<SYS>>\\n\n","You are a helpful, respectful and honest assistant. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n","You are given a context and a question, and your task is to answer the question using the content provided in the context.\n","If the context does not provide content to answer the question, answer: \"impossible to answer\"\n","If context does not provide the content to answer the question,\n","If you don't know the answer to a question, please don't share false information.\n","If a question does not make any sense, explain why instead of answering something not correct.\n","\n","Think step by step and explain your reasoning, then give the answer in JSON format as follows:\n","```json\n","{\n","  \"answer\": ...\n","}\n","```\n","\\n<</SYS>>\\n\\n\n","\"\"\"\n","\n","train_dataset = train_dataset.map(lambda example: {'text': SYSTEM_PROMPT + f\"\"\"Context: {example['context']} \\n\\nQuestion: {example['question']} [\\INST]\\n\n","```json\n","{{\"answer\": {example[\"answers\"][\"text\"]}}}```\n","</s>\"\"\"})\n","print(train_dataset[0])\n","\n","# Step 2 :Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"4d67af37-c36b-4d78-9707-5023d4614946","metadata":{"colab":{"referenced_widgets":["01bfdb97946a4b5d9ad7faabf2a2297e"]},"id":"4d67af37-c36b-4d78-9707-5023d4614946","outputId":"155b2af5-bfd1-4ad2-eaf8-84d85e4e3ca5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01bfdb97946a4b5d9ad7faabf2a2297e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n","You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='32580' max='32580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32580/32580 8:35:00, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.875000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.847200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.837400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.821700</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.801800</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.792600</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.784300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.775000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.761800</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.746600</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.739400</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.723600</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.707400</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.700800</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.679200</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.667900</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.660200</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>0.642400</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>0.639500</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>0.620900</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>0.613000</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>0.598700</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>0.581600</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>0.577300</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>0.564700</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>0.555400</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>0.538500</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>0.534000</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>0.527300</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>0.516800</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>0.502500</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>0.497600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Step 3 :Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","\n","# Step 4 :Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# Step 5 :Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Step 6 :Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Step 7 :Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Step 8 :Train model\n","trainer.train()\n","\n","# Step 9 :Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":null,"id":"46ffad2b-80f5-47c7-ac75-9aaec77ef8ef","metadata":{"id":"46ffad2b-80f5-47c7-ac75-9aaec77ef8ef"},"outputs":[],"source":["model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n","model_to_save.save_pretrained(\"outputs\")"]},{"cell_type":"code","execution_count":null,"id":"3577a2bf-15e6-4a87-ba4e-2779c940a914","metadata":{"colab":{"referenced_widgets":["e2fd729d974043b0b1daaf603fffaa0d"]},"id":"3577a2bf-15e6-4a87-ba4e-2779c940a914","outputId":"356ff1c2-faf3-407a-cf0d-fc525eabb499"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2fd729d974043b0b1daaf603fffaa0d","version_major":2,"version_minor":0},"text/plain":["adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/TANK/Llama-2-7b-chat-hf-squad_v2/commit/507025436cb90878fb4914ae9f8acda8ac632f56', commit_message='Upload model', commit_description='', oid='507025436cb90878fb4914ae9f8acda8ac632f56', pr_url='https://huggingface.co/TANK/Llama-2-7b-chat-hf-squad_v2/discussions/2', pr_revision='refs/pr/2', pr_num=2)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["lora_config = LoraConfig.from_pretrained('outputs')\n","model = get_peft_model(model, lora_config)\n","model.push_to_hub(\"TANK/Llama-2-7b-chat-hf-squad_v2\",create_pr=1)"]},{"cell_type":"code","execution_count":null,"id":"af13034f-6574-4d5f-8759-568df41748d0","metadata":{"id":"af13034f-6574-4d5f-8759-568df41748d0","outputId":"cd3390a3-4389-4bdc-d9ad-22924f1ff637"},"outputs":[{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%load_ext tensorboard\n","%tensorboard --logdir results/runs"]},{"cell_type":"code","execution_count":null,"id":"8bd15e05-f90c-4fb8-94e1-a88374e7dcae","metadata":{"id":"8bd15e05-f90c-4fb8-94e1-a88374e7dcae"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}