{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCREJ/njxIqdFRvsqvABPh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HclXPl7Egm0X"},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n","!pip install scipy\n","!pip install tensorboard\n","!pip install huggingface_hub\n","!huggingface-cli login --token '##############'\n","!pip install tqdm"]},{"cell_type":"code","source":["\n","from tqdm import tqdm\n","import os\n","import torch\n","from datasets import load_dataset, Dataset\n","import transformers\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","\n","import string\n","import re\n","import csv"],"metadata":{"id":"if0x3YYegvdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model_tokenizer(model_name, adapter_name, quantization=False):\n","    if quantization:\n","      bnb_config = BitsAndBytesConfig(\n","      load_in_4bit=True,\n","      bnb_4bit_quant_type=\"nf4\",\n","      bnb_4bit_compute_dtype=\"float16\",\n","      bnb_4bit_use_double_quant=False)\n","    else:\n","      bnb_config = None\n","\n","    model = AutoModelForCausalLM.from_pretrained(model_name, config=bnb_config, device_map=\"auto\")\n","    if adapter_name:\n","      model = PeftModel.from_pretrained(model, adapter_name, device_map=\"auto\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    return model, tokenizer\n","\n","\n","def predict_response(text):\n","  inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","  outputs = model.generate(**inputs, max_new_tokens=50)\n","  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","  return response\n","\n","\n","def normalize_answer(text):\n","    if text:\n","        punc = string.punctuation\n","        text = text.lower()\n","        return ''.join(char for char in text if char not in punc)\n","    else:\n","        return None"],"metadata":{"id":"5hGoDbC_g3z4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"commonsense_qa\"\n","\n","# Load Base model\n","model, tokenizer = load_model_tokenizer(\n","    model_name=model_name,\n","    adapter_name=None,\n","    quantization=True,\n",")\n","device = torch.device('cuda:0')\n","model.to(device)"],"metadata":{"id":"-_muChLQg78o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create validation dataset\n","dataset = load_dataset(dataset_name, split=\"validation\")\n","dataset = dataset.shuffle(seed=1279)\n","dataset = dataset.select(range(200))"],"metadata":{"id":"f7xmytolg-ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Benchmark with only context\n","save_path = 'benchmark_context_commonsenqa.csv'\n","with open(save_path, \"w\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Question\", \"Question concept\", \"Answer key\", \"Choices\", \"Prediction\", \"Full prediction\"])\n","    for i in tqdm(range(len(dataset['question']))):\n","        question = dataset[i]['question']\n","        question_concept = dataset[i]['question_concept']\n","        choices = dataset[i]['choices']\n","        answer_key = dataset[i]['answerKey']\n","        if answer_key:\n","            answer_key = normalize_answer(str(answer_key))\n","        else:\n","            answer = None\n","        promt_choices = ''\n","        for label, text in zip(dataset[i]['choices']['label'], dataset[i]['choices']['text']):\n","            promt_choices += f'{label}: {text}, \\n'\n","\n","        prompt = f\"\"\"\\\n","<s>\n","{question}\n","\n","{promt_choices}\n","\n","ANSWER:\n","``` </s>\"\"\"\n","        full_prediction = predict_response(prompt)\n","        # Find answer\n","        answer_start_index = full_prediction.find(\"ANSWER:\")\n","        prediction = full_prediction[answer_start_index+7:]\n","        pattern = r'\\((\\w)\\)|(?:\\b(\\w):)'\n","        # Find all matches in the text\n","        matches = re.search(pattern, prediction)\n","        if matches:\n","            prediction = matches.group()\n","            prediction = normalize_answer(prediction)\n","        else:\n","            prediction = None\n","        writer.writerow(\n","            [\n","                question,\n","                question_concept,\n","                answer_key,\n","                promt_choices,\n","                prediction,\n","                full_prediction\n","            ]\n","            )\n","        file.flush()\n","\n"],"metadata":{"id":"JzcEXsJ6hMT4"},"execution_count":null,"outputs":[]}]}