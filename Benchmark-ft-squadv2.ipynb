{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3kudQVEBQDkAeNgSdzeuv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AyT4yAMyjttK"},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n","!pip install scipy\n","!pip install tensorboard\n","!pip install huggingface_hub\n","!huggingface-cli login --token '##############'\n","!pip install json5"]},{"cell_type":"code","source":["\n","from tqdm import tqdm\n","import os\n","import torch\n","from datasets import load_dataset, Dataset, load_from_disk\n","import transformers\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","\n","import json5\n","import string\n","import re\n","import csv"],"metadata":{"id":"k-yoz9TDjzMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model_tokenizer(model_name, adapter_name, quantization=False):\n","    if quantization:\n","      bnb_config = BitsAndBytesConfig(\n","      load_in_4bit=True,\n","      bnb_4bit_quant_type=\"nf4\",\n","      bnb_4bit_compute_dtype=\"float16\",\n","      bnb_4bit_use_double_quant=False)\n","    else:\n","      bnb_config = None\n","\n","    model = AutoModelForCausalLM.from_pretrained(model_name, config=bnb_config, device_map=\"auto\")\n","    if adapter_name:\n","      model = PeftModel.from_pretrained(model, adapter_name, device_map=\"auto\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    return model, tokenizer\n","\n","\n","def predict_response(text):\n","  inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","  outputs = model.generate(**inputs, max_new_tokens=100)\n","  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","  return response\n","\n","\n","def normalize_answer(text):\n","    if text:\n","        punc = string.punctuation\n","        text = text.lower()\n","        return ''.join(char for char in text if char not in punc)\n","    else:\n","        return None\n","\n","def extract_final_answer(text):\n","    if text.find(\"}\") != -1 and text.find(\"{\") != -1:\n","        answer = text[text.find(\"{\") : text.find(\"}\")+1]\n","    elif text.find(\"}\") == -1 and text.find(\"{\") != -1:\n","        answer = f\"\"\"\n","        {text[text.find(\"{\") : ]} }}\n","        \"\"\"\n","    elif \"impossible to answer\" in text:\n","        return \"impossible to answer\"\n","\n","    try:\n","        answer = json5.loads(answer)[\"answer\"]\n","    except:\n","        answer = None\n","    return answer"],"metadata":{"id":"GhtgYIdyjzJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","# Fine-tuned model name\n","adapter_name = \"TANK/Llama-2-7b-chat-hf-squad2_v2\"\n","\n","# Base model\n","model, tokenizer = load_model_tokenizer(\n","    model_name=model_name,\n","    adapter_name=adapter_name,\n","    quantization=True,\n",")\n","\n","device = torch.device('cuda:0')\n","model.to(device)"],"metadata":{"id":"H5eX55gkjzG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_from_disk('data/squad_v2/validation')"],"metadata":{"id":"YwOM1ZvrjzEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SYSTEM_PROMPT = \"\"\"\\\n","<s>[INST] <<SYS>>\\n\n","You are a helpful, respectful, and honest assistant. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n","You are given a context and a question, and your task is to answer the question using the content provided in the context.\n","If the context does not provide content to answer the question, answer: \"impossible to answer\"\n","If the context does not provide the content to answer the question,\n","If you don't know the answer to a question, please don't share false information.\n","If a question does not make sense, explain why instead of answering something incorrectly.\n","\n","Think step by step and explain your reasoning, then answer in JSON format as follows:\n","```json\n","{\n","  \"answer\": ...\n","}\n","```\n","\\n<</SYS>>\\n\\n\n","\"\"\"\n","\n","dataset_prompted = dataset.map(lambda example: {'text': SYSTEM_PROMPT + f\"\"\"Context: {example['context']} \\n\\nQuestion: {example['question']} [/INST]\\n\n","```json\n","{{\"answer\": {example[\"answers\"][\"text\"]}}}```\n","</s>\"\"\"})\n","print(dataset_prompted[0]['text'])"],"metadata":{"id":"Cu40wJ9WjzCQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Benchmark with only context\n","save_path = 'benchmark_fine_tuned.csv'\n","with open(save_path, \"w\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Context\", \"Question\", \"Answer\", \"Prediction\", \"Full prediction\"])\n","    for i in tqdm(range(len(dataset_prompted['context']))):\n","        context = dataset_prompted[i]['context']\n","        question = dataset_prompted[i]['question']\n","        if len(dataset_prompted[i]['answers']['text']) == 0:\n","            answer = 'impossible to answer'\n","        else:\n","            answer = normalize_answer(dataset_prompted[i]['answers']['text'][0])\n","\n","        ins_index = dataset_prompted[i]['text'].find('[/INST]\\n')\n","        prompt = dataset_prompted[i]['text'][:ins_index+8] + '\\n</s>'\n","\n","        full_prediction = predict_response(prompt)\n","\n","        answer_start_index = full_prediction.find('[/INST]\\n')\n","        prediction = full_prediction[answer_start_index+9:]\n","\n","        prediction = extract_final_answer(prediction)\n","        prediction = normalize_answer(str(prediction))\n","\n","        writer.writerow(\n","            [\n","                context,\n","                question,\n","                answer,\n","                prediction,\n","                full_prediction\n","            ]\n","            )\n","        file.flush()\n","\n"],"metadata":{"id":"iwTfLyeqjy_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cBQPjhRvjy9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QBTrF09sjy6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8CB2th9qjy4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y-Y_wTE4jy2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f6ebfN2gjyzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RaOruwr-jyxD"},"execution_count":null,"outputs":[]}]}